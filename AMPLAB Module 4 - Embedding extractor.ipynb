{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47YdbbSX6LbT"
   },
   "source": [
    "# AMPLAB Module 4 Machine Listening - Embeddings Extractor\n",
    "\n",
    "This notebook includes the code to extract audio embeddings that could then be used in your other machine listening tasks. This code does not extract embeddings for BSD10k audio files, you'll have to provide your own audio files. If you want to re-analyze BSD10k, you can do so by downloading it. You'll find details in the [BSD10k repository](https://github.com/allholy/BSD10k). You should be able to run this notebook locally or in Google Colab without problems.\n",
    "\n",
    "In order to run this notebook locally, you'll need to create a Python virtual environment and install the requirements (`pip install -r requirements.txt`). Also, you'll need to download the file  `amplab_machine_listening_module_data.zip` that [you'll find in this shared folder](https://drive.google.com/drive/folders/1FHEmzEXgBV1CCAWo_F3KDpw9QM5ecuZf?usp=sharing), and place it uncompressed next to this notebook (the uncompressed folder should be named `amplab_machine_listening_module_data`).\n",
    "\n",
    "If running in Google Colab, you'll need to make a copy of this notebook somewhere in your Google Drive, and add a shortcut to the `amplab_data` shared folder next to your notebook (the shortcut must be named same as the folder, `amplab_data`). Then run the cells normally. Note that before running the first cell, you'll need to update the `%cd ...` path to set the working directory to the folder where the notebook (and the shortcut) are placed within your Google Drive. If in Colab, running the first cell will take some minutes as it needs to copy some data and unzip.\n",
    "\n",
    "This work is similar to that of a paper we published at DCASE 2024:\n",
    "[Anastasopoulou, Panagiota, et al. \"Heterogeneous sound classification with the Broad Sound Taxonomy and Dataset.\" DCASE Workshop (2024)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Anastasopoulou_39.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ytnU7dEIobqT"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  # If this does not fail, it means we're running in a Colab environment\n",
    "\n",
    "  # First mount google drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  # Set the working directory to the directory where this notebook has been placed.\n",
    "  # This directory should have a Google Drive shortcut to the \"amplab_data\" shared folder.\n",
    "  # Edit the below to point to the Google Drive directory where this notebook is located.\n",
    "  %cd '/content/drive/MyDrive/SMC/AMPLab2425/AMPLAB 2025 Module 4 - Machine Listening'\n",
    "\n",
    "  # Now copy data files to the colab runtime local storage and uncompress the .zip file.\n",
    "  # By placing data files in the notebook runtime local storage, we will make data loading much faster in the cells below.\n",
    "  !cp \"amplab_data/amplab_machine_listening_module_data.zip\" /content/amplab_machine_listening_module_data.zip\n",
    "  !unzip  -u /content/amplab_machine_listening_module_data.zip -d /content/\n",
    "  DATA_FOLDER = '/content/amplab_machine_listening_module_data'\n",
    "\n",
    "  # Install dependencies (if not running in Colab, this will need to be installed manually)\n",
    "  !pip install laion_clap essentia-tensorflow\n",
    "\n",
    "except:\n",
    "  # Not running in Colab\n",
    "  DATA_FOLDER = './amplab_machine_listening_module_data'\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import laion_clap\n",
    "import librosa\n",
    "import essentia.standard as estd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "que7tqhKpSbN"
   },
   "source": [
    "# MFCC \"EMBEDDINGS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RmcVWTrDpXZK"
   },
   "outputs": [],
   "source": [
    "mfcc_algo = estd.MFCC()\n",
    "w_algo = estd.Windowing(type='blackmanharris62')\n",
    "spectrum_algo = estd.Spectrum()\n",
    "\n",
    "def get_mfcc_embeddings(audio_path):\n",
    "  loader = estd.MonoLoader(filename=audio_path, sampleRate=48000)\n",
    "  audio = loader()\n",
    "  mfcc_frames = []\n",
    "  for frame in estd.FrameGenerator(audio, frameSize=2048, hopSize=1024):\n",
    "        spec = spectrum_algo(w_algo(frame))\n",
    "        _, mfcc_coeffs = mfcc_algo(spec)\n",
    "        mfcc_frames.append(mfcc_coeffs)\n",
    "  mfcc_frames = np.array(mfcc_frames)\n",
    "  mfcc_average = np.mean(mfcc_frames, axis=0)\n",
    "  return mfcc_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvt4pTpBxuGw"
   },
   "source": [
    "# FREESOUND SIMILARITY EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zfX8fY2OvhZk"
   },
   "outputs": [],
   "source": [
    "gaia_pca_dataset_history = json.load(open(os.path.join(DATA_FOLDER, 'gaia_pca_dataset_history.json')))\n",
    "normalization_coefficients = [transform_info for transform_info in gaia_pca_dataset_history if transform_info[\"Analyzer name\"] == \"normalize\"][0][\"Applier parameters\"][\"coeffs\"]\n",
    "normalization_additional_info = [transform_info for transform_info in gaia_pca_dataset_history if transform_info[\"Analyzer name\"] == \"normalize\"][0][\"Additional info\"]\n",
    "dimensions_per_descriptor = {descriptor_name: len(descriptor_stats['mean']) for descriptor_name, descriptor_stats in normalization_additional_info.items()}\n",
    "pca_descriptor_names = [transform_info for transform_info in gaia_pca_dataset_history if transform_info[\"Analyzer name\"] == \"pca\"][0][\"Applier parameters\"][\"descriptorNames\"]\n",
    "pca_matrix_raw = [transform_info for transform_info in gaia_pca_dataset_history if transform_info[\"Analyzer name\"] == \"pca\"][0][\"Applier parameters\"][\"matrix\"]\n",
    "pca_matrix_raw = pca_matrix_raw[2:]\n",
    "pca_matrix = []\n",
    "for i in range(0, len(pca_matrix_raw), len(pca_matrix_raw)//100):\n",
    "    pca_matrix.append(pca_matrix_raw[i:i+len(pca_matrix_raw)//100])\n",
    "pca_matrix = np.matrix(pca_matrix).transpose()\n",
    "\n",
    "def project_sound_to_legacy_similarity_space(features):\n",
    "    # Normalize\n",
    "    normed_descriptors = {}\n",
    "    for descriptor_name in pca_descriptor_names:\n",
    "        value = features.get(descriptor_name[1:]\n",
    "                             .replace('spectral_contrast.', 'spectral_contrast_coeffs.')\n",
    "                             .replace('scvalleys.', 'spectral_contrast_valleys.')\n",
    "                             .replace('erb_bands.', 'erbbands.')\n",
    "                             .replace('frequency_bands.', 'barkbands.'))  # descriptor names have '.' at the beginning, and some have changed\n",
    "        if type(value) == np.ndarray:\n",
    "            value = list(value)  # Make sure this is not ndarray at this point\n",
    "        if 'frequency_bands.' in descriptor_name:\n",
    "            value += [value[-2]]  # frequency_bands descriptor (which is \"same\" as barkbands?), has one more dimension in the legacy extractor (and that missing dimension seems to be usualy similar to the penultimate)\n",
    "\n",
    "        if type(value) == list:\n",
    "            value_dimensionality = len(value)\n",
    "        else:\n",
    "            value_dimensionality = 1\n",
    "        have_same_dimension = value_dimensionality == dimensions_per_descriptor[descriptor_name]\n",
    "        if value is not None and have_same_dimension:\n",
    "            coeffs = normalization_coefficients[descriptor_name]\n",
    "            if type(value) != list:\n",
    "                norm_value = value * coeffs['a'][0] + coeffs['b'][0]\n",
    "            else:\n",
    "                norm_value = [v * coeffs['a'][i] + coeffs['b'][i] for i, v in enumerate(value)]\n",
    "            normed_descriptors[descriptor_name] = norm_value\n",
    "        else:\n",
    "            # If a descriptor is missing, we set it to 0\n",
    "            # This might (will) happen if some sounds don't have values for all descriptors\n",
    "            #print('Unaligned descriptor', descriptor_name)\n",
    "            if dimensions_per_descriptor[descriptor_name] > 1:\n",
    "                normed_descriptors[descriptor_name] = [0.0 for i in range(0, dimensions_per_descriptor[descriptor_name])]\n",
    "            else:\n",
    "                normed_descriptors[descriptor_name] = 0.0\n",
    "\n",
    "    # Project to pca space\n",
    "    # First concatenate all values into one flat list\n",
    "    vector = []\n",
    "    for descriptor_name in pca_descriptor_names:\n",
    "        normed_value = normed_descriptors[descriptor_name]\n",
    "        if type(normed_value) == list:\n",
    "            vector += normed_value\n",
    "        else:\n",
    "            vector.append(normed_value)\n",
    "    # Then multiply by pca matrix\n",
    "    pca_vector = list(np.squeeze(np.asarray(np.matmul(np.matrix(vector), pca_matrix))))\n",
    "    return pca_vector\n",
    "\n",
    "def get_freesound_similarity_embeddings(audio_path):\n",
    "  fs_pool, _ = estd.FreesoundExtractor()(audio_path)\n",
    "  features = dict()\n",
    "  for descriptor in fs_pool.descriptorNames():\n",
    "      features[descriptor] = fs_pool[descriptor]\n",
    "  sim_vector = project_sound_to_legacy_similarity_space(features)\n",
    "  return np.array(sim_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozsuioEvAKwd"
   },
   "source": [
    "# FSD-SINET EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "95FOGtqUd1iI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 18:26:43.976922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-27 18:26:43.984605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 Laptop GPU computeCapability: 8.6\n",
      "coreClock: 1.402GHz coreCount: 30 deviceMemorySize: 5.70GiB deviceMemoryBandwidth: 268.26GiB/s\n",
      "2025-02-27 18:26:43.985000: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-02-27 18:26:43.985284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2025-02-27 18:26:43.985288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2025-02-27 18:26:43.985291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2025-02-27 18:26:43.995984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-27 18:26:43.998728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 Laptop GPU computeCapability: 8.6\n",
      "coreClock: 1.402GHz coreCount: 30 deviceMemorySize: 5.70GiB deviceMemoryBandwidth: 268.26GiB/s\n",
      "2025-02-27 18:26:43.998737: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-02-27 18:26:43.998744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2025-02-27 18:26:43.998750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2025-02-27 18:26:43.998752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2025-02-27 18:26:44.002077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-27 18:26:44.004849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 Laptop GPU computeCapability: 8.6\n",
      "coreClock: 1.402GHz coreCount: 30 deviceMemorySize: 5.70GiB deviceMemoryBandwidth: 268.26GiB/s\n",
      "2025-02-27 18:26:44.004860: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-02-27 18:26:44.004876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2025-02-27 18:26:44.004880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2025-02-27 18:26:44.004884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "[   INFO   ] TensorflowPredict: Successfully loaded graph file: `./amplab_machine_listening_module_data/fsd-sinet-vgg42-tlpf_aps-1.pb`\n",
      "2025-02-27 18:26:44.089821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-27 18:26:44.092719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 Laptop GPU computeCapability: 8.6\n",
      "coreClock: 1.402GHz coreCount: 30 deviceMemorySize: 5.70GiB deviceMemoryBandwidth: 268.26GiB/s\n",
      "2025-02-27 18:26:44.092732: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-02-27 18:26:44.092740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2025-02-27 18:26:44.092742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2025-02-27 18:26:44.092768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n"
     ]
    }
   ],
   "source": [
    "model_embeddings = estd.TensorflowPredictFSDSINet(graphFilename=os.path.join(DATA_FOLDER, \"fsd-sinet-vgg42-tlpf_aps-1.pb\"), output=\"model/global_max_pooling1d/Max\")\n",
    "\n",
    "def add_silence(audio, sr, silence_duration=0.5):\n",
    "    silence = np.zeros(int(silence_duration * sr))\n",
    "    repeated_audio = np.concatenate((silence, audio))\n",
    "    return repeated_audio\n",
    "\n",
    "def get_fsdsinet_embeddings(audio_path):\n",
    "  loader = estd.MonoLoader(filename=audio_path, sampleRate=44100)\n",
    "  audio = loader()\n",
    "  if len(audio)/44100 < 0.5:\n",
    "    audio = add_silence(audio, 44100)\n",
    "  embeddings = model_embeddings(audio).mean(axis=0)  # Take mean of frame embeddings\n",
    "  return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHt1Fgde_NJp"
   },
   "source": [
    "# CLAP EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNip62NmI6er"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = laion_clap.CLAP_Module(enable_fusion=True)\n",
    "model.load_ckpt(model_id=3) # download the default pretrained checkpoint, this might take some time...\n",
    "\n",
    "def get_clap_embeddings_from_audio(audio_path):\n",
    "    audio, _ = librosa.load(audio_path, sr=48000)\n",
    "    np.random.seed(0)  # Make CLAP's random slice selection for >10s sounds deterministic so we get consistent results when re-run\n",
    "    audio_embed = model.get_audio_embedding_from_data(x=[audio], use_tensor=False)\n",
    "    audio_embed = audio_embed[0, :]\n",
    "    return audio_embed\n",
    "\n",
    "def get_clap_embeddings_from_text(text):\n",
    "    text_embed = model.get_text_embedding([text])\n",
    "    text_embed = text_embed[0, :]\n",
    "    return text_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6PJ0lNy2cJh"
   },
   "source": [
    "# RUN EMBEDDING EXTRACTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uOvZRRmr2a0F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MFCC\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error while configuring MonoLoader: AudioLoader: Could not open file \"amplab_machine_listening_module_data/test_sounds/93100__cgeffex__whip-crack-01.wav\", error = No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m test_sound_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_FOLDER, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_sounds\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m93100__cgeffex__whip-crack-01.wav\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# This should be a valid path to an audio file\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMFCC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m mfcc_embed \u001b[38;5;241m=\u001b[39m \u001b[43mget_mfcc_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_sound_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(mfcc_embed\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(mfcc_embed)\n",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m, in \u001b[0;36mget_mfcc_embeddings\u001b[0;34m(audio_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_mfcc_embeddings\u001b[39m(audio_path):\n\u001b[0;32m----> 6\u001b[0m   loader \u001b[38;5;241m=\u001b[39m \u001b[43mestd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMonoLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampleRate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m48000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m   audio \u001b[38;5;241m=\u001b[39m loader()\n\u001b[1;32m      8\u001b[0m   mfcc_frames \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/amplab4/lib/python3.10/site-packages/essentia/standard.py:44\u001b[0m, in \u001b[0;36m_create_essentia_class.<locals>.Algo.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m _essentia\u001b[38;5;241m.\u001b[39mAlgorithm\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# configure the algorithm\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/amplab4/lib/python3.10/site-packages/essentia/standard.py:64\u001b[0m, in \u001b[0;36m_create_essentia_class.<locals>.Algo.configure\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError cannot convert parameter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\\\n\u001b[1;32m     60\u001b[0m                         \u001b[38;5;241m%\u001b[39m(\u001b[38;5;28mstr\u001b[39m(_c\u001b[38;5;241m.\u001b[39mdetermineEdt(val)),\u001b[38;5;28mstr\u001b[39m(goalType))) \u001b[38;5;66;03m#\\''+name+'\\' parameter: '+str(e))\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     kwargs[name] \u001b[38;5;241m=\u001b[39m convertedVal\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__configure__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error while configuring MonoLoader: AudioLoader: Could not open file \"amplab_machine_listening_module_data/test_sounds/93100__cgeffex__whip-crack-01.wav\", error = No such file or directory"
     ]
    }
   ],
   "source": [
    "test_sound_path = os.path.join(DATA_FOLDER, 'test_sounds', '93100__cgeffex__whip-crack-01.wav')  # This should be a valid path to an audio file\n",
    "\n",
    "print('\\nMFCC')\n",
    "mfcc_embed = get_mfcc_embeddings(test_sound_path)\n",
    "print(mfcc_embed.shape)\n",
    "print(mfcc_embed)\n",
    "\n",
    "print('\\nFreesound similarity')\n",
    "fssim_embed = get_freesound_similarity_embeddings(test_sound_path)\n",
    "print(fssim_embed.shape)\n",
    "print(fssim_embed[0:20])\n",
    "\n",
    "print('\\nFSD-SINET')\n",
    "fsdsinet_embed = get_fsdsinet_embeddings(test_sound_path)\n",
    "print(fsdsinet_embed.shape)\n",
    "print(fsdsinet_embed[0:20])\n",
    "\n",
    "print('\\nCLAP')\n",
    "audio_embed = get_clap_embeddings_from_audio(test_sound_path)\n",
    "print(audio_embed.shape)\n",
    "print(audio_embed[0:20])\n",
    "\n",
    "text_embed = get_clap_embeddings_from_text(\"The sound of a baby crying\")\n",
    "print(text_embed.shape)\n",
    "print(text_embed[0:20])\n",
    "\n",
    "# NOTE: if you wan to do NN search in the CLAP space now, maybe you could use NearestNeighbors from scikit-learn similarly as we did for the audio mosaicing notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5q1IYbmZlkX"
   },
   "source": [
    "# LANGUAGE-BASED AUDIO RETRIEVAL EXAMPLE USING CLAP EMBEDDING SPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3htF7u-aUbBX"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "import sys\n",
    "from IPython.display import display, IFrame\n",
    "\n",
    "def load_embeddings_for_dataset(df, embeddings_folder):\n",
    "  # Returns a numpy array of shape (n, d) where \"n\" is the number of sounds in the dataset and \"d\" is the number of dimensions of the embeddings\n",
    "  # Available embedding types: \"clap\", \"fs_similarity\", \"fsdsinet\", \"mfcc\", \"fsdsinet_frames\", \"mfcc_frames\"\n",
    "  # NOTE: if you are loading embeddings which have been stored frame by frame (i.e. those ending with \"_frames\"), you'll need to add some code\n",
    "  # here to summarize them into a one-dimensional vectors before adding them to the returned numpy array.\n",
    "\n",
    "  base_dir = os.path.join(DATA_FOLDER, 'embeddings', embeddings_folder)\n",
    "  filenames = [os.path.join(base_dir, f'{df.iloc[i][\"sound_id\"]}.npy') for i in range(len(df))]\n",
    "  example_embedding_vector = np.load(filenames[0])\n",
    "  num_dimensions = len(example_embedding_vector)\n",
    "\n",
    "  print(f'Will load {len(filenames)} points of data with {num_dimensions} dimensions each')\n",
    "  X = np.zeros((len(filenames), num_dimensions))\n",
    "  for i, fn in enumerate(filenames):\n",
    "    if (i + 1) % 100 == 0:\n",
    "      sys.stdout.write(f'\\r{i + 1}/{len(filenames)}                  ')\n",
    "      sys.stdout.flush()\n",
    "    X[i, :] = np.load(fn)\n",
    "  sys.stdout.write(f'\\rLoaded {len(filenames)} embeddings from \"{embeddings_folder}\"!')\n",
    "  print()\n",
    "  return X\n",
    "\n",
    "def show_sound_player(sound_id):\n",
    "  display(IFrame(f'https://freesound.org/embed/sound/iframe/{sound_id}/simple/medium/', width=696, height=100))\n",
    "\n",
    "dataset_df = pd.read_csv(open(os.path.join(DATA_FOLDER, 'BSD10k_metadata.csv')))\n",
    "X = load_embeddings_for_dataset(dataset_df, embeddings_folder=\"clap\")\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFiGPRsAVLCX"
   },
   "outputs": [],
   "source": [
    "#target = get_clap_embeddings_from_audio(os.path.join(DATA_FOLDER, 'test_sounds', '93100__cgeffex__whip-crack-01.wav'))\n",
    "#target = get_clap_embeddings_from_audio(os.path.join(DATA_FOLDER, 'test_sounds', '15.wav'))\n",
    "target = get_clap_embeddings_from_text('the sound of a baby crying')\n",
    "distances, indices = nbrs.kneighbors([target])\n",
    "\n",
    "for count, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "  fs_id = dataset_df.iloc[idx][\"sound_id\"]\n",
    "  print(count + 1, '!', distance, fs_id)\n",
    "  show_sound_player(fs_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2_-PD_2WApG"
   },
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit([\n",
    "    get_clap_embeddings_from_text(\"Music\"),  # Music, index 0\n",
    "    get_clap_embeddings_from_text(\"Music bla bla\"),  # Music, index 1\n",
    "    get_clap_embeddings_from_text(\"Sound Effects\"),  # Sound effects, index 2\n",
    "\n",
    "])\n",
    "\n",
    "distances, indices = nbrs.kneighbours([get_clap_embeddings_from_audio('xxx')])\n",
    "indices[0][0] # = 0 = Music\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "amplab4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
